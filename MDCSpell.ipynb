{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 本文内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本文为MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction论文的Pytorch实现。\n",
    "\n",
    "[论文地址](https://aclanthology.org/2022.findings-acl.98/): https://aclanthology.org/2022.findings-acl.98/\n",
    "\n",
    "论文年份：2022\n",
    "\n",
    "[论文笔记](https://blog.csdn.net/zhaohongfei_358/article/details/126973451)：https://blog.csdn.net/zhaohongfei_358/article/details/126973451\n",
    "\n",
    "论文大致内容：作者基于Transformer和BERT设计了一个多任务的网络来进行CSC（Chinese Spell Checking）任务（中文拼写纠错）。多任务分别是找出哪个字是错的和对错字进行纠正。\n",
    "\n",
    "由于作者并没有公开代码，所以我就尝试自己实现一个，最终我的实验结果如下表：\n",
    "\n",
    "| Dataset | Model | D_Precision | D_Recall | D_F1 | C_Prec | C_Rec | C_F1 |\n",
    "|--|--|--|--|--|--|--|--|\n",
    "| SIGHAN 13 | MDCSpell | 89.1 | 78.3| 83.4 | 87.5| 76.8 | 81.8 |\n",
    "| SIGHAN 13 | MDCSpell(复现) | 80.2 | 79.9| 80.0 | 77.2| 76.9 | 77.1 |\n",
    "| SIGHAN 14 | MDCSpell | 70.2 | 68.8| 69.5 | 69.0| 67.7 | 68.3 |\n",
    "| SIGHAN 14 | MDCSpell(复现) | 82.8 | 66.6| 73.8 | 79.9| 64.3 | 71.2 |\n",
    "| SIGHAN 15 | MDCSpell | 80.8 | 80.6| 80.7 | 78.4| 78.2 | 78.3 |\n",
    "| SIGHAN 15 | MDCSpell(复现) | 86.7 | 76.1| 81.1 | 72.5| 82.7 | 77.3 |\n",
    "\n",
    "这里是我训练了2个epoch的结果，与作者的结论相差不大。如果我增加训练次数的话，也许可以和作者的结果达到一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except:\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'1.12.1+cu113'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'4.21.3'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 句子的长度，作者并没有说明。我这里就按经验取一个\n",
    "max_length = 128\n",
    "# 作者使用的batch_size\n",
    "batch_size = 32\n",
    "# epoch数，作者并没有具体说明，按经验取一个\n",
    "epochs = 10\n",
    "\n",
    "# 每${log_after_step}步，打印一次日志\n",
    "log_after_step = 20\n",
    "\n",
    "# 模型存放的位置。\n",
    "model_path = './drive/MyDrive/models/MDCSpell/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "# 第一阶段的模型存储路径（使用Wang271K训练）\n",
    "model_path = model_path + 'MDCSpell-model.pt'\n",
    "# 第二阶段的模型存储路径（使用SIGHAN训练集进行fine-tune）\n",
    "final_model_path = model_path.replace(\"MDCSpell-model.pt\", \"MDCSpell-model-final.pt\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"asserts/images/model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Correction Network** 的数据流向如下：\n",
    "\n",
    "1.将token序列 `[CLS] 遇 到 逆 竟 [SEP]` 送给Word Embedding模块进行embeddings，得到向量$\\{e_{CLS}^w, e_1^w, e_2^w, e_3^w, e_4^w,e_{SEP}^w\\}$。\n",
    "\n",
    "> 个人认为此时的embedding仅仅是Word Embeding，并不包含Position Embedding和Segment Embedding。\n",
    "\n",
    "2.之后将$\\{e_{CLS}^w, e_1^w, e_2^w, e_3^w, e_4^w,e_{SEP}^w\\}$向量送入BERT，增加Position Embedding和Segment Embedding，得到 $\\{e_C, e_1, e_2, e_3, e_4, e_S\\}$。\n",
    "\n",
    "3.在BERT内部，会经历多层的TransformerEncoder，最终的得到输出向量 $H^c=\\{h_C^c, h_1^c, h_2^c, h_3^c, h_4^c, h_S^c\\}$.\n",
    "\n",
    "4.将BERT的输出 $H^c$ 和 隔壁Detection Network输出的 $H^d$ 进行融合，得到 $H = H^d+H^c$\n",
    "\n",
    "> 融合时并不对`[CLS]`和`[SEP]`进行融合\n",
    "\n",
    "5.将$H$送给全连接层(Dense Layer)做最后的预测。\n",
    "\n",
    "<br>\n",
    "\n",
    "**Correction Network模型细节**：\n",
    "\n",
    "1. **BERT**：作者使用的是具有12层Transformer Block的BERT-base版。\n",
    "2. **Dense Layer**：Dense Layer的输入通道为词向量维度，输出通道为词典大小。例如：词向量维度为768，词典大小为20000，则Dense Layer则为`nn.Linear(768, 20000)`\n",
    "3. **Dense Layer的初始化**：Dense Layer的权重使用的是Word Embedding的参数。因为word Embedding是将词index转成词向量，所以其参数刚好是Dense Layer的转置，即Word Embedding是`nn.Linear(20000, 768)`，所以作者就是用Word Embedding的转置来初始化Dense Layer的参数。因为这样可以加速训练，且使模型变的稳定。\n",
    "\n",
    "---\n",
    "\n",
    "**Detection Network**的数据流向如下：\n",
    "\n",
    "1.输入为使用BERT得到的word Embedding $\\{e_1^w, e_2^w, e_3^w, e_4^w\\}$。虽然图里并不包含`[CLS]`和`[SEP]`的词向量，但个人认为不需要对其特殊处理，因为最后的预测也用不到这两个token.\n",
    "\n",
    "2.将$\\{e_1^w, e_2^w, e_3^w, e_4^w\\}$增加Position Embedding信息，得到$\\{e_1', e_2', e_3', e_4'\\}$\n",
    "\n",
    "> 在论文中说Detection Network使用的是向量$\\{e_1, e_2, e_3, e_4\\}$，其是word embedding+position embedding+segment embedding。这与图上是矛盾的，这里以图为准了。\n",
    "\n",
    "3.将$\\{e_1', e_2', e_3', e_4'\\}$向量送入Transformer Block，得到输出向量$H^d=\\{h_1^d, h_2^d, h_3^d, h_4^d\\}$\n",
    "\n",
    "4.一方面，将输出向量$H^d$送给隔壁的Correction Network进行融合；另一方面，将$H^d$送给后续的全连接层(Dense Layer)来判断哪个token是错误的.\n",
    "\n",
    "**Detection Network**的细节：\n",
    "\n",
    "1. **Transformer Block**：Transformer Block是2层的TransformerEncoder。\n",
    "2. **Transformer Block参数初始化**：Transformer Block参数初始化使用的是BERT的权重。\n",
    "3. **Dense Layer**：Dense Layer的输入通道为词向量大小，输出通道为1。使用Sigmoid来判别该token为错字的概率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CorrectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CorrectionNetwork, self).__init__()\n",
    "        # BERT分词器，作者并没提到自己使用的是哪个中文版的bert，我这里就使用一个比较常用的\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT\n",
    "        self.bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT的word embedding，本质就是个nn.Embedding\n",
    "        self.word_embedding_table = self.bert.get_input_embeddings()\n",
    "        # 预测层。hidden_size是词向量的大小，len(self.tokenizer)是词典大小\n",
    "        self.dense_layer = nn.Linear(self.bert.config.hidden_size, len(self.tokenizer))\n",
    "\n",
    "    def forward(self, inputs, word_embeddings, detect_hidden_states):\n",
    "        \"\"\"\n",
    "        Correction Network的前向传递\n",
    "        :param inputs: inputs为tokenizer对中文文本的分词结果，\n",
    "                       里面包含了token对一个的index，attention_mask等\n",
    "        :param word_embeddings: 使用BERT的word_embedding对token进行embedding后的结果\n",
    "        :param detect_hidden_states: Detection Network输出hidden state\n",
    "        :return: Correction Network对个token的预测结果。\n",
    "        \"\"\"\n",
    "        # 1. 使用bert进行前向传递\n",
    "        bert_outputs = self.bert(token_type_ids=inputs['token_type_ids'],\n",
    "                                 attention_mask=inputs['attention_mask'],\n",
    "                                 inputs_embeds=word_embeddings)\n",
    "        # 2. 将bert的hidden_state和Detection Network的hidden state进行融合。\n",
    "        hidden_states = bert_outputs['last_hidden_state'] + detect_hidden_states\n",
    "        # 3. 最终使用全连接层进行token预测\n",
    "        return self.dense_layer(hidden_states)\n",
    "\n",
    "    def get_inputs_and_word_embeddings(self, sequences, max_length=128):\n",
    "        \"\"\"\n",
    "        对中文序列进行分词和word embeddings处理\n",
    "        :param sequences: 中文文本序列。例如: [\"鸡你太美\", \"哎呦，你干嘛！\"]\n",
    "        :param max_length: 文本的最大长度，不足则进行填充，超出进行裁剪。\n",
    "        :return: tokenizer的输出和word embeddings.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(sequences, padding='max_length', max_length=max_length, return_tensors='pt',\n",
    "                                truncation=True).to(device)\n",
    "        # 使用BERT的work embeddings对token进行embedding，这里得到的embedding并不包含position embedding和segment embedding\n",
    "        word_embeddings = self.word_embedding_table(inputs['input_ids'])\n",
    "        return inputs, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DetectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, position_embeddings, transformer_blocks, hidden_size):\n",
    "        \"\"\"\n",
    "        :param position_embeddings: bert的position_embeddings，本质是一个nn.Embedding\n",
    "        :param transformer: BERT的前两层transformer_block，其是一个ModuleList对象\n",
    "        \"\"\"\n",
    "        super(DetectionNetwork, self).__init__()\n",
    "        self.position_embeddings = position_embeddings\n",
    "        self.transformer_blocks = transformer_blocks\n",
    "\n",
    "        # 定义最后的预测层，预测哪个token是错误的\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        # 获取token序列的长度，这里为128\n",
    "        sequence_length = word_embeddings.size(1)\n",
    "        # 生成position embedding\n",
    "        position_embeddings = self.position_embeddings(torch.LongTensor(range(sequence_length)).to(device))\n",
    "        # 融合work_embedding和position_embedding\n",
    "        x = word_embeddings + position_embeddings\n",
    "        # 将x一层一层的使用transformer encoder进行向后传递\n",
    "        for transformer_layer in self.transformer_blocks:\n",
    "            x = transformer_layer(x)[0]\n",
    "\n",
    "        # 最终返回Detection Network输出的hidden states和预测结果\n",
    "        hidden_states = x\n",
    "        return hidden_states, self.dense_layer(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MDCSpellModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MDCSpellModel, self).__init__()\n",
    "        # 构造Correction Network\n",
    "        self.correction_network = CorrectionNetwork()\n",
    "        self._init_correction_dense_layer()\n",
    "\n",
    "        # 构造Detection Network\n",
    "        # position embedding使用BERT的\n",
    "        position_embeddings = self.correction_network.bert.embeddings.position_embeddings\n",
    "        # 作者在论文中提到的，Detection Network的Transformer使用BERT的权重\n",
    "        # 所以我这里直接克隆BERT的前两层Transformer来完成这个动作\n",
    "        transformer = copy.deepcopy(self.correction_network.bert.encoder.layer[:2])\n",
    "        # 提取BERT的词向量大小\n",
    "        hidden_size = self.correction_network.bert.config.hidden_size\n",
    "\n",
    "        # 构造Detection Network\n",
    "        self.detection_network = DetectionNetwork(position_embeddings, transformer, hidden_size)\n",
    "\n",
    "    def forward(self, sequences, max_length=128):\n",
    "        # 先获取word embedding，Correction Network和Detection Network都要用\n",
    "        inputs, word_embeddings = self.correction_network.get_inputs_and_word_embeddings(sequences, max_length)\n",
    "        # Detection Network进行前向传递，获取输出的Hidden State和预测结果\n",
    "        hidden_states, detection_outputs = self.detection_network(word_embeddings)\n",
    "        # Correction Network进行前向传递，获取其预测结果\n",
    "        correction_outputs = self.correction_network(inputs, word_embeddings, hidden_states)\n",
    "        # 返回Correction Network 和 Detection Network 的预测结果。\n",
    "        # 在计算损失时`[PAD]`token不需要参与计算，所以这里将`[PAD]`部分全都变为0\n",
    "        return correction_outputs, detection_outputs.squeeze(2) * inputs['attention_mask']\n",
    "\n",
    "    def _init_correction_dense_layer(self):\n",
    "        \"\"\"\n",
    "        原论文中提到，使用Word Embedding的weight来对Correction Network进行初始化\n",
    "        \"\"\"\n",
    "        self.correction_network.dense_layer.weight.data = self.correction_network.word_embedding_table.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义好模型后，我们来简单的尝试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction_outputs shape: torch.Size([2, 128, 21128])\n",
      "detection_outputs shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MDCSpellModel().to(device)\n",
    "correction_outputs, detection_outputs = model([\"鸡你太美\", \"哎呦，你干嘛！\"])\n",
    "print(\"correction_outputs shape:\", correction_outputs.size())\n",
    "print(\"detection_outputs shape:\", detection_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Correction Network和Detection Network使用的都是Cross Entropy。之后进行相加即可：\n",
    "\n",
    "$$\n",
    "L = \\lambda L^c + (1-\\lambda) L^d\n",
    "$$\n",
    "\n",
    "其中 $\\lambda \\in [0,1]$ 。作者通过实验得出 $\\lambda=0.85$ 时效果最好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MDCSpellLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, coefficient=0.85):\n",
    "        super(MDCSpellLoss, self).__init__()\n",
    "        # 定义Correction Network的Loss函数\n",
    "        self.correction_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # 定义Detection Network的Loss函数，因为是二分类，所以用Binary Cross Entropy\n",
    "        self.detection_criterion = nn.BCELoss()\n",
    "        # 权重系数\n",
    "        self.coefficient = coefficient\n",
    "\n",
    "    def forward(self, correction_outputs, correction_targets, detection_outputs, detection_targets):\n",
    "        \"\"\"\n",
    "        :param correction_outputs: Correction Network的输出，Shape为(batch_size, sequence_length, hidden_size)\n",
    "        :param correction_targets: Correction Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :param detection_outputs: Detection Network的输出，Shape为(batch_size, sequence_length)\n",
    "        :param detection_targets: Detection Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 计算Correction Network的loss，因为Shape维度为3，所以要把batch_size和sequence_length进行合并才能计算\n",
    "        correction_loss = self.correction_criterion(correction_outputs.view(-1, correction_outputs.size(2)),\n",
    "                                                    correction_targets.view(-1))\n",
    "        # 计算Detection Network的loss\n",
    "        detection_loss = self.detection_criterion(detection_outputs, detection_targets)\n",
    "        # 对两个loss进行加权平均\n",
    "        return self.coefficient * correction_loss + (1 - self.coefficient) * detection_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "作者的训练方式：\n",
    "\n",
    "1. 第一步，首先使用 [Wang271K(自己造的假数据)](https://github.com/wdimmy/Automatic-Corpus-Generation) 数据集进行训练。batch size为32， learning rate为2e-5\n",
    "\n",
    "2. 第二步，使用SIGHAN训练集进行fine-tune。 batch size为32，learning rate为1e-5\n",
    "\n",
    "> 作者并没有提到使用的是什么Optimizer，但看这个学习率，应该是Adam。\n",
    "\n",
    "> 在第一步，作者说的是使用了几乎3M个，但作者只提到过Wang271K这个数据集，我猜可能作者看错了，这个是0.3M条数据，而不是3M。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "作者首先使用了Wang271K数据集进行对模型进行训练，然后又使用SIGHAN训练集对模型进行fine-tune。这里我就不进行fine-tune了，直接进行训练。我这里使用的是 [ReaLiSe](https://github.com/DaDaMrX/ReaLiSe)论文 处理好的数据集，其就是Wang271K和SIGHAN。\n",
    "\n",
    "[SIGHAN百度网盘链接](https://pan.baidu.com/s/1t8LnxZYaNngV-ofxoPL-Vw?pwd=oon2) ：https://pan.baidu.com/s/1t8LnxZYaNngV-ofxoPL-Vw?pwd=oon2\n",
    "\n",
    "[Wang271K百度网盘链接](https://pan.baidu.com/s/1iUe4mvkcAyWi6VDVD8EATw?pwd=d65c) : https://pan.baidu.com/s/1iUe4mvkcAyWi6VDVD8EATw?pwd=d65c\n",
    "\n",
    "下载好直接解压即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gdown' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "系统找不到指定的路径。\n",
      "'gdown' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!gdown '1CMNG1F_MmdrC1VGDpY6S0LCdLZcUuhhC' --output  \"sighan.zip\"\n",
    "!unzip sighan.zip > /dev/null\n",
    "!gdown '1K76OK6muaLo3pYvPw5vhnNRUre996C_T' --output 'Wang271K_processed.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 构造Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CSCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        super(CSCDataset, self).__init__()\n",
    "        with open(filename, mode='br') as f:\n",
    "            train_data = pickle.load(f)\n",
    "\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.train_data[index]['src']\n",
    "        tgt = self.train_data[index]['tgt']\n",
    "        return src, tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CSCDataset('Wang271K_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('联合国紧急事务首席协调官艾蒽兰表示，这是全球有史以来首次子灾难发生候这么短一段时间内，就筹集到这么高的金额。',\n '联合国紧急事务首席协调官艾基兰表示，这是全球有史以来首次在灾难发生后这么短一段时间内，就筹集到这么高的金额。')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在使用Wang271K训练模型后，作者还会使用SIGHAN训练集对模型进行fine-tune:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "sighan_13_train_data = CSCDataset('sighan/Train/sighan13_training_set_simplified.pkl')\n",
    "sighan_14_train_data = CSCDataset('sighan/Train/sighan14_training_set_simplified.pkl')\n",
    "sighan_15_train_data = CSCDataset('sighan/Train/sighan15_training_set_simplified.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(700, 3437, 2339)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sighan_13_train_data), len(sighan_14_train_data), len(sighan_15_train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 构造Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src, tgt = list(src), list(tgt)\n",
    "\n",
    "    src_tokens = tokenizer(src, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "    tgt_tokens = tokenizer(tgt, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "\n",
    "    correction_targets = tgt_tokens\n",
    "    detection_targets = (src_tokens != tgt_tokens).float()\n",
    "    return src, correction_targets, detection_targets, src_tokens  # src_tokens在计算Correction的精准率时要用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "sighan_train_loader = DataLoader(ConcatDataset([sighan_13_train_data, sighan_14_train_data, sighan_15_train_data]), batch_size=batch_size, collate_fn=collate_fn, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = MDCSpellLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "start_epoch = 0  # 从哪个epoch开始\n",
    "total_step = 0  # 一共更新了多少次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢复训练，epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# 恢复之前的训练\n",
    "if os.path.exists(model_path):\n",
    "    if not torch.cuda.is_available():\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    total_step = checkpoint['total_step']\n",
    "    print(\"恢复训练，epoch:\", start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练这里代码量看起来很大，但实际大多都是计算recall和precision的代码。这里对于Detection的recall和precision的计算使用的是Detection Network的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, total_step, model_path, start_epoch=0):\n",
    "    model = model.train()\n",
    "    total_loss = 0.  # 记录loss\n",
    "\n",
    "    d_recall_numerator = 0  # Detection的Recall的分子\n",
    "    d_recall_denominator = 0  # Detection的Recall的分母\n",
    "    d_precision_numerator = 0  # Detection的precision的分子\n",
    "    d_precision_denominator = 0  # Detection的precision的分母\n",
    "    c_recall_numerator = 0  # Correction的Recall的分子\n",
    "    c_recall_denominator = 0  # Correction的Recall的分母\n",
    "    c_precision_numerator = 0  # Correction的precision的分子\n",
    "    c_precision_denominator = 0  # Correction的precision的分母\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        for sequences, correction_targets, detection_targets, correction_inputs in train_loader:\n",
    "            correction_targets, detection_targets = correction_targets.to(device), detection_targets.to(device)\n",
    "            correction_inputs = correction_inputs.to(device)\n",
    "            correction_outputs, detection_outputs = model(sequences)\n",
    "            loss = criterion(correction_outputs, correction_targets, detection_outputs, detection_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "            # 计算Detection的recall和precision指标\n",
    "            # 大于0.5，认为是错误token，反之为正确token\n",
    "            d_predicts = detection_outputs >= 0.5\n",
    "            # 计算错误token中被网络正确预测到的数量\n",
    "            d_recall_numerator += d_predicts[detection_targets == 1].sum().item()\n",
    "            # 计算错误token的数量\n",
    "            d_recall_denominator += (detection_targets == 1).sum().item()\n",
    "            # 计算网络预测的错误token的数量\n",
    "            d_precision_denominator += d_predicts.sum().item()\n",
    "            # 计算网络预测的错误token中，有多少是真错误的token\n",
    "            d_precision_numerator += (detection_targets[d_predicts == 1]).sum().item()\n",
    "\n",
    "            # 计算Correction的recall和precision\n",
    "            # 将输出映射成index，即将correction_outputs的Shape由(32, 128, 21128)变为(32,128)\n",
    "            correction_outputs = correction_outputs.argmax(2)\n",
    "            # 对于填充、[CLS]和[SEP]这三个token不校验\n",
    "            correction_outputs[(correction_targets == 0) | (correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "            # correction_targets的[CLS]和[SEP]也要变为0\n",
    "            correction_targets[(correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "            # Correction的预测结果，其中True表示预测正确，False表示预测错误或无需预测\n",
    "            c_predicts = correction_outputs == correction_targets\n",
    "            # 计算错误token中被网络正确纠正的token数量\n",
    "            c_recall_numerator += c_predicts[detection_targets == 1].sum().item()\n",
    "            # 计算错误token的数量\n",
    "            c_recall_denominator += (detection_targets == 1).sum().item()\n",
    "            # 计算网络纠正token的数量\n",
    "            correction_inputs[(correction_inputs == 101) | (correction_inputs == 102)] = 0\n",
    "            c_precision_denominator += (correction_outputs != correction_inputs).sum().item()\n",
    "            # 计算在网络纠正的这些token中，有多少是真正被纠正对的\n",
    "            c_precision_numerator += c_predicts[correction_outputs != correction_inputs].sum().item()\n",
    "\n",
    "            if total_step % log_after_step == 0:\n",
    "                loss = total_loss / log_after_step\n",
    "                d_recall = d_recall_numerator / (d_recall_denominator + 1e-9)\n",
    "                d_precision = d_precision_numerator / (d_precision_denominator + 1e-9)\n",
    "                c_recall = c_recall_numerator / (c_recall_denominator + 1e-9)\n",
    "                c_precision = c_precision_numerator / (c_precision_denominator + 1e-9)\n",
    "\n",
    "                print(\"Epoch {}, \"\n",
    "                      \"Step {}/{}, \"\n",
    "                      \"Total Step {}, \"\n",
    "                      \"loss {:.5f}, \"\n",
    "                      \"detection recall {:.4f}, \"\n",
    "                      \"detection precision {:.4f}, \"\n",
    "                      \"correction recall {:.4f}, \"\n",
    "                      \"correction precision {:.4f}\".format(epoch, step, len(train_loader), total_step,\n",
    "                                                           loss,\n",
    "                                                           d_recall,\n",
    "                                                           d_precision,\n",
    "                                                           c_recall,\n",
    "                                                           c_precision))\n",
    "\n",
    "                total_loss = 0.\n",
    "                d_recall_numerator = 0\n",
    "                d_recall_denominator = 0\n",
    "                d_precision_numerator = 0\n",
    "                d_precision_denominator = 0\n",
    "                c_recall_numerator = 0\n",
    "                c_recall_denominator = 0\n",
    "                c_precision_numerator = 0\n",
    "                c_precision_denominator = 0\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'total_step': total_step,\n",
    "        }, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "作者将训练分为两个阶段，第一个阶段使用Wang271K数据集对模型进行训练，第二阶段使用SIGHAN训练集对模型进行fine-tune。这里我先进行第一阶段的训练："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 15/8479, Total Step 8900, loss 0.02588, detection recall 0.3921, detection precision 0.8302, correction recall 0.7959, correction precision 0.9365\n",
      "Epoch 2, Step 35/8479, Total Step 8920, loss 0.03279, detection recall 0.3934, detection precision 0.8018, correction recall 0.8120, correction precision 0.9322\n",
      "Epoch 2, Step 55/8479, Total Step 8940, loss 0.03126, detection recall 0.4109, detection precision 0.8599, correction recall 0.8173, correction precision 0.9243\n",
      "Epoch 2, Step 75/8479, Total Step 8960, loss 0.03574, detection recall 0.4252, detection precision 0.8292, correction recall 0.8120, correction precision 0.9146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-22-3b5442b2b016>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-21-0b520b2ddf5f>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, train_loader, total_step, model_path)\u001B[0m\n\u001B[0;32m     20\u001B[0m             \u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_targets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    394\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    395\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 396\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    397\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    398\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[1;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, total_step, model_path, start_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在使用Wang271K数据训练完毕后，使用SIGHAN训练集对模型进行fine-tune:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 8.00 GiB total capacity; 7.20 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-39-52bb7d39f031>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mfine_tune_optimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1e-5\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# fine-tune时作者使用的是1e-5的学习率\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfine_tune_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msighan_train_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinal_model_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-35-39ea18e1589f>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, train_loader, total_step, model_path, start_epoch)\u001B[0m\n\u001B[0;32m     19\u001B[0m             \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_targets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_targets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m             \u001B[0mcorrection_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcorrection_inputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m             \u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_targets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-8-51aa7067347c>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, sequences, max_length)\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetection_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_embeddings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;31m# Correction Network进行前向传递，获取其预测结果\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[0mcorrection_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorrection_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mword_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[1;31m# 返回Correction Network 和 Detection Network 的预测结果。\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[1;31m# 在计算损失时`[PAD]`token不需要参与计算，所以这里将`[PAD]`部分全都变为0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-6-e3493711de67>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, inputs, word_embeddings, detect_hidden_states)\u001B[0m\n\u001B[0;32m     22\u001B[0m         \"\"\"\n\u001B[0;32m     23\u001B[0m         \u001B[1;31m# 1. 使用bert进行前向传递\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m         bert_outputs = self.bert(token_type_ids=inputs['token_type_ids'],\n\u001B[0m\u001B[0;32m     25\u001B[0m                                  \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'attention_mask'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m                                  inputs_embeds=word_embeddings)\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1016\u001B[0m             \u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpast_key_values_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1017\u001B[0m         )\n\u001B[1;32m-> 1018\u001B[1;33m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[0;32m   1019\u001B[0m             \u001B[0membedding_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1020\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    605\u001B[0m                 )\n\u001B[0;32m    606\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 607\u001B[1;33m                 layer_outputs = layer_module(\n\u001B[0m\u001B[0;32m    608\u001B[0m                     \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m                     \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    533\u001B[0m             \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpresent_key_value\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcross_attn_present_key_value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 535\u001B[1;33m         layer_output = apply_chunking_to_forward(\n\u001B[0m\u001B[0;32m    536\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchunk_size_feed_forward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseq_len_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    537\u001B[0m         )\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py\u001B[0m in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput_chunks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunk_dim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 243\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mforward_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput_tensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    244\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    245\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mfeed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    545\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    546\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfeed_forward_chunk\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 547\u001B[1;33m         \u001B[0mintermediate_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    548\u001B[0m         \u001B[0mlayer_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintermediate_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_output\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlayer_output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    446\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    447\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 448\u001B[1;33m         \u001B[0mhidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintermediate_act_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    449\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\activations.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mact\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     57\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 8.00 GiB total capacity; 7.20 GiB already allocated; 0 bytes free; 7.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "fine_tune_optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) # fine-tune时作者使用的是1e-5的学习率\n",
    "train(model, fine_tune_optimizer, sighan_train_loader, 0, final_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "在原论文中，作者给出的指标是Sentence-Level的，即以句子为最小单位，一句话话都修改对才算对，否则就算预测失败。但不少论文（例如SpellGCN）给出了character-level的评估结果，所以这里我也给出来，做个参考。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# 加载训练后的模型\n",
    "if os.path.exists(final_model_path):\n",
    "    if not torch.cuda.is_available():\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model = model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(\"sighan/Test/sighan13_test_set_simplified.pkl\", mode='br') as f:\n",
    "    sighan13 = pickle.load(f)\n",
    "\n",
    "with open(\"sighan/Test/sighan14_test_set_simplified.pkl\", mode='br') as f:\n",
    "    sighan14 = pickle.load(f)\n",
    "\n",
    "with open(\"sighan/Test/sighan15_test_set_simplified.pkl\", mode='br') as f:\n",
    "    sighan15 = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Character-level Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型评估使用了SIGHAN 2013,2014,2015三个数据集对模型进行评估。对于Detection的Precision和Recall的评估，使用的是Correction Network的结果，这和训练阶段有所不同，这是因为Detection Network只是帮助Correction Network训练的，其结果在使用时不具备参考价值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(test_data):\n",
    "    d_recall_numerator = 0  # Detection的Recall的分子\n",
    "    d_recall_denominator = 0  # Detection的Recall的分母\n",
    "    d_precision_numerator = 0  # Detection的precision的分子\n",
    "    d_precision_denominator = 0  # Detection的precision的分母\n",
    "    c_recall_numerator = 0  # Correction的Recall的分子\n",
    "    c_recall_denominator = 0  # Correction的Recall的分母\n",
    "    c_precision_numerator = 0  # Correction的precision的分子\n",
    "    c_precision_denominator = 0  # Correction的precision的分母\n",
    "\n",
    "    prograss = tqdm(range(len(test_data)))\n",
    "    for i in prograss:\n",
    "        src, tgt = test_data[i]['src'], test_data[i]['tgt']\n",
    "\n",
    "        src_tokens = tokenizer(src, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "        tgt_tokens = tokenizer(tgt, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "\n",
    "        # 正常情况下，src和tgt的长度应该是一致的\n",
    "        if len(src_tokens) != len(tgt_tokens):\n",
    "            print(\"第%d条数据异常\" % i)\n",
    "            continue\n",
    "\n",
    "        correction_outputs, _ = model(src)\n",
    "        predict_tokens = correction_outputs[0][1:len(src_tokens) + 1].argmax(1).detach().cpu()\n",
    "\n",
    "        # 计算错误token的数量\n",
    "        d_recall_denominator += (src_tokens != tgt_tokens).sum().item()\n",
    "        # 计算在这些错误token，有多少网络也认为它是错误的\n",
    "        d_recall_numerator += (predict_tokens != src_tokens)[src_tokens != tgt_tokens].sum().item()\n",
    "        # 计算网络找出的错误token的数量\n",
    "        d_precision_denominator += (predict_tokens != src_tokens).sum().item()\n",
    "        # 计算在网络找出的这些错误token中，有多少是真正错误的\n",
    "        d_precision_numerator += (src_tokens != tgt_tokens)[predict_tokens != src_tokens].sum().item()\n",
    "        # 计算Detection的recall、precision和f1-score\n",
    "        d_recall = d_recall_numerator / (d_recall_denominator + 1e-9)\n",
    "        d_precision = d_precision_numerator / (d_precision_denominator + 1e-9)\n",
    "        d_f1_score = 2 * (d_recall * d_precision) / (d_recall + d_precision + 1e-9)\n",
    "\n",
    "        # 计算错误token的数量\n",
    "        c_recall_denominator += (src_tokens != tgt_tokens).sum().item()\n",
    "        # 计算在这些错误token中，有多少网络预测对了\n",
    "        c_recall_numerator += (predict_tokens == tgt_tokens)[src_tokens != tgt_tokens].sum().item()\n",
    "        # 计算网络找出的错误token的数量\n",
    "        c_precision_denominator += (predict_tokens != src_tokens).sum().item()\n",
    "        # 计算网络找出的错误token中，有多少是正确修正的\n",
    "        c_precision_numerator += (predict_tokens == tgt_tokens)[predict_tokens != src_tokens].sum().item()\n",
    "\n",
    "        # 计算Correction的recall、precision和f1-score\n",
    "        c_recall = c_recall_numerator / (c_recall_denominator + 1e-9)\n",
    "        c_precision = c_precision_numerator / (c_precision_denominator + 1e-9)\n",
    "        c_f1_score = 2 * (c_recall * c_precision) / (c_recall + c_precision + 1e-9)\n",
    "\n",
    "        prograss.set_postfix({\n",
    "            'd_recall': d_recall,\n",
    "            'd_precision': d_precision,\n",
    "            'd_f1_score': d_f1_score,\n",
    "            'c_recall': c_recall,\n",
    "            'c_precision': c_precision,\n",
    "            'c_f1_score': c_f1_score,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 98.11it/s, d_recall=0.77, d_precision=0.78, d_f1_score=0.775, c_recall=0.752, c_precision=0.762, c_f1_score=0.757]  \n"
     ]
    }
   ],
   "source": [
    "evaluation(sighan13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:10<00:00, 105.92it/s, d_recall=0.647, d_precision=0.808, d_f1_score=0.718, c_recall=0.63, c_precision=0.787, c_f1_score=0.699] \n"
     ]
    }
   ],
   "source": [
    "evaluation(sighan14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:11<00:00, 97.76it/s, d_recall=0.724, d_precision=0.835, d_f1_score=0.776, c_recall=0.697, c_precision=0.804, c_f1_score=0.747] \n"
     ]
    }
   ],
   "source": [
    "evaluation(sighan15)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence-level Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "作者使用的评估指标是Sentence-level的，所以我这里需要使用sentence-level结果与原论文进行比较。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def sentence_level_evaluation(test_data):\n",
    "    \"\"\"\n",
    "    :param test_data: sighan13,14,15数据\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    d_tp: Detection True Positive. 如果Label句子中有错误，且预测的错字index和Label完全一致，则d_tp+1\n",
    "    d_fp: Detection False Positive. 如果Label句子中没有错误，但预测的结果中认为该句子有错误，则d_fp+1\n",
    "    d_tn: Detection True Negative. 如果Label句子中没有错误，且预测结果也认为该句子没有错误，则d_tn+1\n",
    "    d_fn: Detection False Negative. 如果Label句子中有错误，但预测的错字index与Label不一致，则d_fn+1\n",
    "    \"\"\"\n",
    "    d_tp, d_fp, d_tn, d_fn = 0, 0, 0, 0\n",
    "\n",
    "    \"\"\"\n",
    "    c_tp: Correction True Positive. 如果Label句子中有错误，且修改后的句子和Label完全一致，则c_tp+1\n",
    "    c_fp: Correction False Positive. 如果Label句子中没有错误，但预测的结果中认为该句子有错误，则c_fp+1，该值与d_fp一致。\n",
    "    c_tn: Correction True Negative. 如果Label句子中没有错误，且预测结果也认为该句子没有错误，则c_tn+1，该值与d_tn一致\n",
    "    c_fn: Correction False Negative. 如果Label句子中有错误，但预测结果与其不一致，则c_fn+1\n",
    "    \"\"\"\n",
    "    c_tp, c_fp, c_tn, c_fn = 0, 0, 0, 0\n",
    "\n",
    "    # 记录每个指标的具体题目\n",
    "    detail_dict = {\n",
    "        \"d_tp\": [], \"d_fp\": [], \"d_tn\": [], \"d_fn\": [],\n",
    "        \"c_tp\": [], \"c_fp\": [], \"c_tn\": [], \"c_fn\": [],\n",
    "    }\n",
    "\n",
    "    prograss = tqdm(range(len(test_data)))\n",
    "    for i in prograss:\n",
    "        id, src, tgt = test_data[i]['id'], test_data[i]['src'], test_data[i]['tgt']\n",
    "\n",
    "        src_tokens = tokenizer(src, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "        tgt_tokens = tokenizer(tgt, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "\n",
    "        # 正常情况下，src和tgt的长度应该是一致的\n",
    "        if len(src_tokens) != len(tgt_tokens):\n",
    "            print(\"第%d条数据异常\" % i)\n",
    "            continue\n",
    "\n",
    "        correction_outputs, _ = model(src)\n",
    "        predict_tokens = correction_outputs[0][1:len(src_tokens) + 1].argmax(1).detach().cpu()\n",
    "\n",
    "        # Label中是否有错误\n",
    "        label_has_error = (src_tokens != tgt_tokens).sum().item() > 0\n",
    "        # Detection是否预测正确\n",
    "        d_pred_is_correct = ((predict_tokens != src_tokens) != (src_tokens != tgt_tokens)).sum().item() <= 0\n",
    "\n",
    "        # 如果Label句子中有错误，且预测的错字index和Label完全一致，则d_tp+1\n",
    "        if label_has_error and d_pred_is_correct:\n",
    "            d_tp += 1\n",
    "            detail_dict['d_tp'].append(id)\n",
    "\n",
    "        # 如果Label句子中没有错误，但预测的结果中认为该句子有错误，则d_fp+1\n",
    "        if not label_has_error and not d_pred_is_correct:\n",
    "            d_fp += 1\n",
    "            detail_dict['d_fp'].append(id)\n",
    "            detail_dict['c_fp'].append(id)\n",
    "\n",
    "        # 如果Label句子中没有错误，且预测结果也认为该句子没有错误，则d_tn+1\n",
    "        if not label_has_error and d_pred_is_correct:\n",
    "            d_tn += 1\n",
    "            detail_dict['d_tn'].append(id)\n",
    "            detail_dict['c_tn'].append(id)\n",
    "\n",
    "        # 如果Label句子中有错误，但预测结果与其不一致，则d_fn+1\n",
    "        if label_has_error and not d_pred_is_correct:\n",
    "            d_fn += 1\n",
    "            detail_dict['d_fn'].append(id)\n",
    "\n",
    "        c_pred_is_correct = (predict_tokens != tgt_tokens).sum().item() <= 0\n",
    "        # Correction True Positive. 如果Label句子中有错误，且修改后的句子和Label完全一致，则c_tp+1\n",
    "        if label_has_error and c_pred_is_correct:\n",
    "            c_tp += 1\n",
    "            detail_dict['c_tp'].append(id)\n",
    "\n",
    "        # 如果Label句子中没有错误，但预测的结果中认为该句子有错误，则c_fp+1，该值与d_fp一致。\n",
    "        c_fp = d_fp\n",
    "        # 如果Label句子中没有错误，且预测结果也认为该句子没有错误，则c_tn+1，该值与d_tn一致\n",
    "        c_tn = d_tn\n",
    "        # 如果Label句子中有错误，但预测结果与其不一致，则c_fn+1\n",
    "        if label_has_error and not c_pred_is_correct:\n",
    "            c_fn += 1\n",
    "            detail_dict['c_fn'].append(id)\n",
    "\n",
    "        # 计算Detection的recall、precision和f1-score\n",
    "        d_accuracy = (d_tp + d_tn) / (d_tp + d_tn + d_fp + d_fn + 1e-9)\n",
    "        d_precision = d_tp / (d_tp + d_fp + 1e-9)\n",
    "        d_recall = d_tp / (d_tp + d_fn + 1e-9)\n",
    "        d_f1_score = 2 * (d_recall * d_precision) / (d_recall + d_precision + 1e-9)\n",
    "\n",
    "        # 计算Correction的recall、precision和f1-score\n",
    "        c_accuracy = (c_tp + c_tn) / (c_tp + c_tn + c_fp + c_fn + 1e-9)\n",
    "        c_precision = c_tp / (c_tp + c_fp + 1e-9)\n",
    "        c_recall = c_tp / (c_tp + c_fn + 1e-9)\n",
    "        c_f1_score = 2 * (c_recall * c_precision) / (c_recall + c_precision + 1e-9)\n",
    "\n",
    "        prograss.set_postfix({\n",
    "            'd_accuracy': '%.3f(%d/%d)' % (d_accuracy, (d_tp + d_tn), (d_tp + d_tn + d_fp + d_fn)),\n",
    "            'd_precision': '%.3f(%d/%d)' % (d_precision, d_tp, d_tp + d_fp),\n",
    "            'd_recall': '%.3f(%d/%d)' % (d_recall, d_tp, d_tp + d_fn),\n",
    "            'd_f1_score': d_f1_score,\n",
    "            'c_accuracy': '%.3f(%d/%d)' % (c_accuracy, (c_tp + c_tn), (c_tp + c_tn + c_fp + c_fn)),\n",
    "            'c_precision': '%.3f(%d/%d)' % (c_precision, c_tp, c_tp + c_fp),\n",
    "            'c_recall': '%.3f(%d/%d)' % (c_recall, c_tp, c_tp + c_fn),\n",
    "            'c_f1_score': c_f1_score,\n",
    "        })\n",
    "\n",
    "    return detail_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 118.57it/s, d_accuracy=0.573(573/1000), d_precision=0.986(551/559), d_recall=0.568(551/970), d_f1_score=0.721, c_accuracy=0.560(560/1000), c_precision=0.985(538/546), c_recall=0.555(538/970), c_f1_score=0.71]\n"
     ]
    }
   ],
   "source": [
    "sighan13_detail = sentence_level_evaluation(sighan13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:08<00:00, 120.39it/s, d_accuracy=0.729(774/1062), d_precision=0.825(293/355), d_recall=0.565(293/519), d_f1_score=0.67, c_accuracy=0.719(764/1062), c_precision=0.820(283/345), c_recall=0.545(283/519), c_f1_score=0.655] \n"
     ]
    }
   ],
   "source": [
    "sighan14_detail = sentence_level_evaluation(sighan14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:09<00:00, 122.12it/s, d_accuracy=0.796(876/1100), d_precision=0.894(361/404), d_recall=0.666(361/542), d_f1_score=0.763, c_accuracy=0.787(866/1100), c_precision=0.891(351/394), c_recall=0.648(351/542), c_f1_score=0.75] \n"
     ]
    }
   ],
   "source": [
    "sighan15_detail = sentence_level_evaluation(sighan15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SIGHAN结果导出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SIGHAN数据集提供了Sentence-level的评估检测工具，所以我们这里也导出SIGHAN工具要求的数据结构，然后使用SIGHAN工具进行评估。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def export_prediction(test_data, output_filename):\n",
    "    lines = []\n",
    "    prograss = tqdm(range(len(test_data)))\n",
    "    for i in prograss:\n",
    "        src, tgt, id = test_data[i]['src'], test_data[i]['tgt'], test_data[i]['id']\n",
    "\n",
    "        src_tokens = tokenizer(src, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "        tgt_tokens = tokenizer(tgt, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "\n",
    "        # 正常情况下，src和tgt的长度应该是一致的\n",
    "        if len(src_tokens) != len(tgt_tokens):\n",
    "            print(\"第%d条数据异常\" % i)\n",
    "            continue\n",
    "\n",
    "        correction_outputs, _ = model(src)\n",
    "        predict_tokens = correction_outputs[0][1:len(src_tokens) + 1].argmax(1).detach().cpu()\n",
    "\n",
    "        # 找出哪些地方有错误\n",
    "        detection_indices = torch.where(predict_tokens != src_tokens)[0]\n",
    "        correction_tokens = tokenizer.convert_ids_to_tokens(predict_tokens[detection_indices])\n",
    "\n",
    "        line = id\n",
    "        if len(detection_indices) <= 0:\n",
    "            line += ', 0'\n",
    "        else:\n",
    "            for j, index in enumerate(detection_indices):\n",
    "                line += ', %d, %s' % (index.item() + 1, correction_tokens[j])\n",
    "        line += '\\n'\n",
    "        lines.append(line)\n",
    "\n",
    "    os.mkdir(\"output\") if not os.path.exists(\"output\") else ''\n",
    "    with open(output_filename, mode='w', encoding='utf-8') as f:\n",
    "        f.writelines(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 107.32it/s]\n"
     ]
    }
   ],
   "source": [
    "export_prediction(sighan13, 'output/sighan13_predict.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:08<00:00, 119.91it/s]\n"
     ]
    }
   ],
   "source": [
    "export_prediction(sighan14, 'output/sighan14_predict.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:09<00:00, 117.46it/s]\n"
     ]
    }
   ],
   "source": [
    "export_prediction(sighan15, 'output/sighan15_predict.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "完成后可以使用如下命令查看结果（需要安装java）：\n",
    "\n",
    "```\n",
    "java -jar asserts/tools/sighan15csc.jar -i output/sighan13_predict.csv -t sighan/Test/sighan13/FinalTest_SubTask2_Truth.txt\n",
    "java -jar asserts/tools/sighan15csc.jar -i output/sighan14_predict.csv -t sighan/Test/sighan14/CLP14_CSC_TestTruth.txt\n",
    "java -jar asserts/tools/sighan15csc.jar -i output/sighan15_predict.csv -t sighan/Test/sighan15/SIGHAN15_CSC_TestTruth.txt\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "最后，我们来真正的使用一下该模型，看下效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    sequences = [text]\n",
    "    correction_outputs, _ = model(sequences)\n",
    "    tokens = correction_outputs[0][1:len(text) + 1].argmax(1)\n",
    "    return ''.join(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'今天早上我吃了一个火聋果'"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"今天早上我吃了以个火聋果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'我是联系时长两年半的个人练习生蔡徐鲲，喜欢唱跳ra##p蓝球[SEP]'"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"我是联系时长两年半的个人练习生蔡徐鲲，喜欢唱跳RAP蓝球\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "虽然在数据上模型表现还不错，但在真正使用场景上，效果还是不够好。中文文本纠错果然是一个比较难的任务 T_T !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 参考文献\n",
    "\n",
    "[MDCSpell论文](https://aclanthology.org/2022.findings-acl.98/): https://aclanthology.org/2022.findings-acl.98/\n",
    "\n",
    "[MDCSpell论文笔记](https://blog.csdn.net/zhaohongfei_358/article/details/126973451)：https://blog.csdn.net/zhaohongfei_358/article/details/126973451"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}