{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 本文内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "本文为MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction论文的Pytorch实现。\n",
    "\n",
    "[论文地址](https://aclanthology.org/2022.findings-acl.98/): https://aclanthology.org/2022.findings-acl.98/\n",
    "\n",
    "论文年份：2022\n",
    "\n",
    "[论文笔记](https://blog.csdn.net/zhaohongfei_358/article/details/126973451)：https://blog.csdn.net/zhaohongfei_358/article/details/126973451\n",
    "\n",
    "论文大致内容：作者基于Transformer和BERT设计了一个多任务的网络来进行CSC（Chinese Spell Checking）任务（中文拼写纠错）。多任务分别是找出哪个字是错的和对错字进行纠正。\n",
    "\n",
    "由于作者并没有公开代码，所以我就尝试自己实现一个，最终我的实验结果如下表：\n",
    "\n",
    "| Dataset | Model | D_Precision | D_Recall | D_F1 | C_Prec | C_Rec | C_F1 |\n",
    "|--|--|--|--|--|--|--|--|\n",
    "| SIGHAN 13 | MDCSpell | 89.1 | 78.3| 83.4 | 87.5| 76.8 | 81.8 |\n",
    "| SIGHAN 13 | MDCSpell(复现) | 80.2 | 79.9| 80.0 | 77.2| 76.9 | 77.1 |\n",
    "| SIGHAN 14 | MDCSpell | 70.2 | 68.8| 69.5 | 69.0| 67.7 | 68.3 |\n",
    "| SIGHAN 14 | MDCSpell(复现) | 82.8 | 66.6| 73.8 | 79.9| 64.3 | 71.2 |\n",
    "| SIGHAN 15 | MDCSpell | 80.8 | 80.6| 80.7 | 78.4| 78.2 | 78.3 |\n",
    "| SIGHAN 15 | MDCSpell(复现) | 86.7 | 76.1| 81.1 | 72.5| 82.7 | 77.3 |\n",
    "\n",
    "这里是我训练了2个epoch的结果，与作者的结论相差不大。如果我增加训练次数的话，也许可以和作者的结果达到一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except:\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.21.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 句子的长度，作者并没有说明。我这里就按经验取一个\n",
    "max_length = 128\n",
    "# 作者使用的batch_size\n",
    "batch_size = 32\n",
    "# epoch数，作者并没有具体说明，按经验取一个\n",
    "epochs = 10\n",
    "\n",
    "# 每${log_after_step}步，打印一次日志\n",
    "log_after_step = 20\n",
    "\n",
    "# 模型存放的位置。\n",
    "model_path = './drive/MyDrive/models/MDCSpell/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_path = model_path + 'MDCSpell-model.pt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"asserts/images/model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Correction Network** 的数据流向如下：\n",
    "\n",
    "1.将token序列 `[CLS] 遇 到 逆 竟 [SEP]` 送给Word Embedding模块进行embeddings，得到向量$\\{e_{CLS}^w, e_1^w, e_2^w, e_3^w, e_4^w,e_{SEP}^w\\}$。\n",
    "\n",
    "> 个人认为此时的embedding仅仅是Word Embeding，并不包含Position Embedding和Segment Embedding。\n",
    "\n",
    "2.之后将$\\{e_{CLS}^w, e_1^w, e_2^w, e_3^w, e_4^w,e_{SEP}^w\\}$向量送入BERT，增加Position Embedding和Segment Embedding，得到 $\\{e_C, e_1, e_2, e_3, e_4, e_S\\}$。\n",
    "\n",
    "3.在BERT内部，会经历多层的TransformerEncoder，最终的得到输出向量 $H^c=\\{h_C^c, h_1^c, h_2^c, h_3^c, h_4^c, h_S^c\\}$.\n",
    "\n",
    "4.将BERT的输出 $H^c$ 和 隔壁Detection Network输出的 $H^d$ 进行融合，得到 $H = H^d+H^c$\n",
    "\n",
    "> 融合时并不对`[CLS]`和`[SEP]`进行融合\n",
    "\n",
    "5.将$H$送给全连接层(Dense Layer)做最后的预测。\n",
    "\n",
    "<br>\n",
    "\n",
    "**Correction Network模型细节**：\n",
    "\n",
    "1. **BERT**：作者使用的是具有12层Transformer Block的BERT-base版。\n",
    "2. **Dense Layer**：Dense Layer的输入通道为词向量维度，输出通道为词典大小。例如：词向量维度为768，词典大小为20000，则Dense Layer则为`nn.Linear(768, 20000)`\n",
    "3. **Dense Layer的初始化**：Dense Layer的权重使用的是Word Embedding的参数。因为word Embedding是将词index转成词向量，所以其参数刚好是Dense Layer的转置，即Word Embedding是`nn.Linear(20000, 768)`，所以作者就是用Word Embedding的转置来初始化Dense Layer的参数。因为这样可以加速训练，且使模型变的稳定。\n",
    "\n",
    "---\n",
    "\n",
    "**Detection Network**的数据流向如下：\n",
    "\n",
    "1.输入为使用BERT得到的word Embedding $\\{e_1^w, e_2^w, e_3^w, e_4^w\\}$。虽然图里并不包含`[CLS]`和`[SEP]`的词向量，但个人认为不需要对其特殊处理，因为最后的预测也用不到这两个token.\n",
    "\n",
    "2.将$\\{e_1^w, e_2^w, e_3^w, e_4^w\\}$增加Position Embedding信息，得到$\\{e_1', e_2', e_3', e_4'\\}$\n",
    "\n",
    "> 在论文中说Detection Network使用的是向量$\\{e_1, e_2, e_3, e_4\\}$，其是word embedding+position embedding+segment embedding。这与图上是矛盾的，这里以图为准了。\n",
    "\n",
    "3.将$\\{e_1', e_2', e_3', e_4'\\}$向量送入Transformer Block，得到输出向量$H^d=\\{h_1^d, h_2^d, h_3^d, h_4^d\\}$\n",
    "\n",
    "4.一方面，将输出向量$H^d$送给隔壁的Correction Network进行融合；另一方面，将$H^d$送给后续的全连接层(Dense Layer)来判断哪个token是错误的.\n",
    "\n",
    "**Detection Network**的细节：\n",
    "\n",
    "1. **Transformer Block**：Transformer Block是2层的TransformerEncoder。\n",
    "2. **Transformer Block参数初始化**：Transformer Block参数初始化使用的是BERT的权重。\n",
    "3. **Dense Layer**：Dense Layer的输入通道为词向量大小，输出通道为1。使用Sigmoid来判别该token为错字的概率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CorrectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CorrectionNetwork, self).__init__()\n",
    "        # BERT分词器，作者并没提到自己使用的是哪个中文版的bert，我这里就使用一个比较常用的\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT\n",
    "        self.bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT的word embedding，本质就是个nn.Embedding\n",
    "        self.word_embedding_table = self.bert.get_input_embeddings()\n",
    "        # 预测层。hidden_size是词向量的大小，len(self.tokenizer)是词典大小\n",
    "        self.dense_layer = nn.Linear(self.bert.config.hidden_size, len(self.tokenizer))\n",
    "\n",
    "    def forward(self, inputs, word_embeddings, detect_hidden_states):\n",
    "        \"\"\"\n",
    "        Correction Network的前向传递\n",
    "        :param inputs: inputs为tokenizer对中文文本的分词结果，\n",
    "                       里面包含了token对一个的index，attention_mask等\n",
    "        :param word_embeddings: 使用BERT的word_embedding对token进行embedding后的结果\n",
    "        :param detect_hidden_states: Detection Network输出hidden state\n",
    "        :return: Correction Network对个token的预测结果。\n",
    "        \"\"\"\n",
    "        # 1. 使用bert进行前向传递\n",
    "        bert_outputs = self.bert(token_type_ids=inputs['token_type_ids'],\n",
    "                                 attention_mask=inputs['attention_mask'],\n",
    "                                 inputs_embeds=word_embeddings)\n",
    "        # 2. 将bert的hidden_state和Detection Network的hidden state进行融合。\n",
    "        hidden_states = bert_outputs['last_hidden_state'] + detect_hidden_states\n",
    "        # 3. 最终使用全连接层进行token预测\n",
    "        return self.dense_layer(hidden_states)\n",
    "\n",
    "    def get_inputs_and_word_embeddings(self, sequences, max_length=128):\n",
    "        \"\"\"\n",
    "        对中文序列进行分词和word embeddings处理\n",
    "        :param sequences: 中文文本序列。例如: [\"鸡你太美\", \"哎呦，你干嘛！\"]\n",
    "        :param max_length: 文本的最大长度，不足则进行填充，超出进行裁剪。\n",
    "        :return: tokenizer的输出和word embeddings.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(sequences, padding='max_length', max_length=max_length, return_tensors='pt',\n",
    "                                truncation=True).to(device)\n",
    "        # 使用BERT的work embeddings对token进行embedding，这里得到的embedding并不包含position embedding和segment embedding\n",
    "        word_embeddings = self.word_embedding_table(inputs['input_ids'])\n",
    "        return inputs, word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DetectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, position_embeddings, transformer_blocks, hidden_size):\n",
    "        \"\"\"\n",
    "        :param position_embeddings: bert的position_embeddings，本质是一个nn.Embedding\n",
    "        :param transformer: BERT的前两层transformer_block，其是一个ModuleList对象\n",
    "        \"\"\"\n",
    "        super(DetectionNetwork, self).__init__()\n",
    "        self.position_embeddings = position_embeddings\n",
    "        self.transformer_blocks = transformer_blocks\n",
    "\n",
    "        # 定义最后的预测层，预测哪个token是错误的\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        # 获取token序列的长度，这里为128\n",
    "        sequence_length = word_embeddings.size(1)\n",
    "        # 生成position embedding\n",
    "        position_embeddings = self.position_embeddings(torch.LongTensor(range(sequence_length)).to(device))\n",
    "        # 融合work_embedding和position_embedding\n",
    "        x = word_embeddings + position_embeddings\n",
    "        # 将x一层一层的使用transformer encoder进行向后传递\n",
    "        for transformer_layer in self.transformer_blocks:\n",
    "            x = transformer_layer(x)[0]\n",
    "\n",
    "        # 最终返回Detection Network输出的hidden states和预测结果\n",
    "        hidden_states = x\n",
    "        return hidden_states, self.dense_layer(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MDCSpellModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MDCSpellModel, self).__init__()\n",
    "        # 构造Correction Network\n",
    "        self.correction_network = CorrectionNetwork()\n",
    "        self._init_correction_dense_layer()\n",
    "\n",
    "        # 构造Detection Network\n",
    "        # position embedding使用BERT的\n",
    "        position_embeddings = self.correction_network.bert.embeddings.position_embeddings\n",
    "        # 作者在论文中提到的，Detection Network的Transformer使用BERT的权重\n",
    "        # 所以我这里直接克隆BERT的前两层Transformer来完成这个动作\n",
    "        transformer = copy.deepcopy(self.correction_network.bert.encoder.layer[:2])\n",
    "        # 提取BERT的词向量大小\n",
    "        hidden_size = self.correction_network.bert.config.hidden_size\n",
    "\n",
    "        # 构造Detection Network\n",
    "        self.detection_network = DetectionNetwork(position_embeddings, transformer, hidden_size)\n",
    "\n",
    "    def forward(self, sequences, max_length=128):\n",
    "        # 先获取word embedding，Correction Network和Detection Network都要用\n",
    "        inputs, word_embeddings = self.correction_network.get_inputs_and_word_embeddings(sequences, max_length)\n",
    "        # Detection Network进行前向传递，获取输出的Hidden State和预测结果\n",
    "        hidden_states, detection_outputs = self.detection_network(word_embeddings)\n",
    "        # Correction Network进行前向传递，获取其预测结果\n",
    "        correction_outputs = self.correction_network(inputs, word_embeddings, hidden_states)\n",
    "        # 返回Correction Network 和 Detection Network 的预测结果。\n",
    "        # 在计算损失时`[PAD]`token不需要参与计算，所以这里将`[PAD]`部分全都变为0\n",
    "        return correction_outputs, detection_outputs.squeeze(2) * inputs['attention_mask']\n",
    "\n",
    "    def _init_correction_dense_layer(self):\n",
    "        \"\"\"\n",
    "        原论文中提到，使用Word Embedding的weight来对Correction Network进行初始化\n",
    "        \"\"\"\n",
    "        self.correction_network.dense_layer.weight.data = self.correction_network.word_embedding_table.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "定义好模型后，我们来简单的尝试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction_outputs shape: torch.Size([2, 128, 21128])\n",
      "detection_outputs shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MDCSpellModel().to(device)\n",
    "correction_outputs, detection_outputs = model([\"鸡你太美\", \"哎呦，你干嘛！\"])\n",
    "print(\"correction_outputs shape:\", correction_outputs.size())\n",
    "print(\"detection_outputs shape:\", detection_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Correction Network和Detection Network使用的都是Cross Entropy。之后进行相加即可：\n",
    "\n",
    "$$\n",
    "L = \\lambda L^c + (1-\\lambda) L^d\n",
    "$$\n",
    "\n",
    "其中 $\\lambda \\in [0,1]$ 。作者通过实验得出 $\\lambda=0.85$ 时效果最好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MDCSpellLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, coefficient=0.85):\n",
    "        super(MDCSpellLoss, self).__init__()\n",
    "        # 定义Correction Network的Loss函数\n",
    "        self.correction_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # 定义Detection Network的Loss函数，因为是二分类，所以用Binary Cross Entropy\n",
    "        self.detection_criterion = nn.BCELoss()\n",
    "        # 权重系数\n",
    "        self.coefficient = coefficient\n",
    "\n",
    "    def forward(self, correction_outputs, correction_targets, detection_outputs, detection_targets):\n",
    "        \"\"\"\n",
    "        :param correction_outputs: Correction Network的输出，Shape为(batch_size, sequence_length, hidden_size)\n",
    "        :param correction_targets: Correction Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :param detection_outputs: Detection Network的输出，Shape为(batch_size, sequence_length)\n",
    "        :param detection_targets: Detection Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 计算Correction Network的loss，因为Shape维度为3，所以要把batch_size和sequence_length进行合并才能计算\n",
    "        correction_loss = self.correction_criterion(correction_outputs.view(-1, correction_outputs.size(2)),\n",
    "                                                    correction_targets.view(-1))\n",
    "        # 计算Detection Network的loss\n",
    "        detection_loss = self.detection_criterion(detection_outputs, detection_targets)\n",
    "        # 对两个loss进行加权平均\n",
    "        return self.coefficient * correction_loss + (1 - self.coefficient) * detection_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "作者的训练方式：\n",
    "\n",
    "1. 第一步，首先使用 [Wang271K(自己造的假数据)](https://github.com/wdimmy/Automatic-Corpus-Generation) 数据集进行训练。batch size为32， learning rate为2e-5\n",
    "\n",
    "2. 第二步，使用SIGHAN训练集进行fine-tune。 batch size为32，learning rate为1e-5\n",
    "\n",
    "> 作者并没有提到使用的是什么Optimizer，但看这个学习率，应该是Adam。\n",
    "\n",
    "> 在第一步，作者说的是使用了几乎3M个，但作者只提到过Wang271K这个数据集，我猜可能作者看错了，这个是0.3M条数据，而不是3M。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "作者首先使用了Wang271K数据集进行对模型进行训练，然后又使用SIGHAN训练集对模型进行fine-tune。这里我就不进行fine-tune了，直接进行训练。我这里使用的是 [ReaLiSe](https://github.com/DaDaMrX/ReaLiSe)论文 处理好的数据集，其就是Wang271K和SIGHAN。\n",
    "\n",
    "[百度网盘链接](https://pan.baidu.com/s/1x67LPiYAjLKhO1_2CI6aOA?pwd=skda) ：https://pan.baidu.com/s/1x67LPiYAjLKhO1_2CI6aOA?pwd=skda\n",
    "\n",
    "下载好直接解压即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gdown' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!gdown '1dC09i57lobL91lEbpebDuUBS0fGz-LAk' --folder --output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 构造Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CSCDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CSCDataset, self).__init__()\n",
    "        with open(\"data/trainall.times2.pkl\", mode='br') as f:\n",
    "            train_data = pickle.load(f)\n",
    "\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.train_data[index]['src']\n",
    "        tgt = self.train_data[index]['tgt']\n",
    "        return src, tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CSCDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('纽约早盘作为基准的低硫轻油，五月份交割价攀升一点三四美元，来到每桶二十八点二五美元，而上周五曾下挫一豪元以上。',\n",
       " '纽约早盘作为基准的低硫轻油，五月份交割价攀升一点三四美元，来到每桶二十八点二五美元，而上周五曾下挫一美元以上。')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 构造Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src, tgt = list(src), list(tgt)\n",
    "\n",
    "    src_tokens = tokenizer(src, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "    tgt_tokens = tokenizer(tgt, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "\n",
    "    correction_targets = tgt_tokens\n",
    "    detection_targets = (src_tokens != tgt_tokens).float()\n",
    "    return src, correction_targets, detection_targets, src_tokens  # src_tokens在计算Correction的精准率时要用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = MDCSpellLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "start_epoch = 0  # 从哪个epoch开始\n",
    "total_step = 0  # 一共更新了多少次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢复训练，epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# 恢复之前的训练\n",
    "if os.path.exists(model_path):\n",
    "    if not torch.cuda.is_available():\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    total_step = checkpoint['total_step']\n",
    "    print(\"恢复训练，epoch:\", start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练这里代码量看起来很大，但实际大多都是计算recall和precision的代码。这里对于Detection的recall和precision的计算使用的是Detection Network的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 15/8882, Total Step 8900, loss 0.02403, detection recall 0.4118, detection precision 0.8247, correction recall 0.8192, correction precision 0.9485\n",
      "Epoch 2, Step 35/8882, Total Step 8920, loss 0.03479, detection recall 0.3658, detection precision 0.8055, correction recall 0.8029, correction precision 0.9125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-c9af61235927>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msequences\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdetection_targets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    394\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    395\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 396\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    397\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    398\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    171\u001B[0m     \u001B[1;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m     \u001B[1;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "total_loss = 0.  # 记录loss\n",
    "\n",
    "d_recall_numerator = 0  # Detection的Recall的分子\n",
    "d_recall_denominator = 0  # Detection的Recall的分母\n",
    "d_precision_numerator = 0  # Detection的precision的分子\n",
    "d_precision_denominator = 0  # Detection的precision的分母\n",
    "c_recall_numerator = 0  # Correction的Recall的分子\n",
    "c_recall_denominator = 0  # Correction的Recall的分母\n",
    "c_precision_numerator = 0  # Correction的precision的分子\n",
    "c_precision_denominator = 0  # Correction的precision的分母\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for sequences, correction_targets, detection_targets, correction_inputs in train_loader:\n",
    "        correction_targets, detection_targets = correction_targets.to(device), detection_targets.to(device)\n",
    "        correction_inputs = correction_inputs.to(device)\n",
    "        correction_outputs, detection_outputs = model(sequences)\n",
    "        loss = criterion(correction_outputs, correction_targets, detection_outputs, detection_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        # 计算Detection的recall和precision指标\n",
    "        # 大于0.5，认为是错误token，反之为正确token\n",
    "        d_predicts = detection_outputs >= 0.5\n",
    "        # 计算错误token中被网络正确预测到的数量\n",
    "        d_recall_numerator += d_predicts[detection_targets == 1].sum().item()\n",
    "        # 计算错误token的数量\n",
    "        d_recall_denominator += (detection_targets == 1).sum().item()\n",
    "        # 计算网络预测的错误token的数量\n",
    "        d_precision_denominator += d_predicts.sum().item()\n",
    "        # 计算网络预测的错误token中，有多少是真错误的token\n",
    "        d_precision_numerator += (detection_targets[d_predicts == 1]).sum().item()\n",
    "\n",
    "        # 计算Correction的recall和precision\n",
    "        # 将输出映射成index，即将correction_outputs的Shape由(32, 128, 21128)变为(32,128)\n",
    "        correction_outputs = correction_outputs.argmax(2)\n",
    "        # 对于填充、[CLS]和[SEP]这三个token不校验\n",
    "        correction_outputs[(correction_targets == 0) | (correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "        # correction_targets的[CLS]和[SEP]也要变为0\n",
    "        correction_targets[(correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "        # Correction的预测结果，其中True表示预测正确，False表示预测错误或无需预测\n",
    "        c_predicts = correction_outputs == correction_targets\n",
    "        # 计算错误token中被网络正确纠正的token数量\n",
    "        c_recall_numerator += c_predicts[detection_targets == 1].sum().item()\n",
    "        # 计算错误token的数量\n",
    "        c_recall_denominator += (detection_targets == 1).sum().item()\n",
    "        # 计算网络纠正token的数量\n",
    "        correction_inputs[(correction_inputs == 101) | (correction_inputs == 102)] = 0\n",
    "        c_precision_denominator += (correction_outputs != correction_inputs).sum().item()\n",
    "        # 计算在网络纠正的这些token中，有多少是真正被纠正对的\n",
    "        c_precision_numerator += c_predicts[correction_outputs != correction_inputs].sum().item()\n",
    "\n",
    "        if total_step % log_after_step == 0:\n",
    "            loss = total_loss / log_after_step\n",
    "            d_recall = d_recall_numerator / (d_recall_denominator + 1e-9)\n",
    "            d_precision = d_precision_numerator / (d_precision_denominator + 1e-9)\n",
    "            c_recall = c_recall_numerator / (c_recall_denominator + 1e-9)\n",
    "            c_precision = c_precision_numerator / (c_precision_denominator + 1e-9)\n",
    "\n",
    "            print(\"Epoch {}, \"\n",
    "                  \"Step {}/{}, \"\n",
    "                  \"Total Step {}, \"\n",
    "                  \"loss {:.5f}, \"\n",
    "                  \"detection recall {:.4f}, \"\n",
    "                  \"detection precision {:.4f}, \"\n",
    "                  \"correction recall {:.4f}, \"\n",
    "                  \"correction precision {:.4f}\".format(epoch, step, len(train_loader), total_step,\n",
    "                                                       loss,\n",
    "                                                       d_recall,\n",
    "                                                       d_precision,\n",
    "                                                       c_recall,\n",
    "                                                       c_precision))\n",
    "\n",
    "            total_loss = 0.\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "            d_recall_numerator = 0\n",
    "            d_recall_denominator = 0\n",
    "            d_precision_numerator = 0\n",
    "            d_precision_denominator = 0\n",
    "            c_recall_numerator = 0\n",
    "            c_recall_denominator = 0\n",
    "            c_precision_numerator = 0\n",
    "            c_precision_denominator = 0\n",
    "\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'total_step': total_step,\n",
    "    }, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型评估使用了SIGHAN 2013,2014,2015三个数据集对模型进行评估。对于Detection的Precision和Recall的评估，使用的是Correction Network的结果，这和训练阶段有所不同，这是因为Detection Network只是帮助Correction Network训练的，其结果在使用时不具备参考价值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(test_data):\n",
    "    d_recall_numerator = 0  # Detection的Recall的分子\n",
    "    d_recall_denominator = 0  # Detection的Recall的分母\n",
    "    d_precision_numerator = 0  # Detection的precision的分子\n",
    "    d_precision_denominator = 0  # Detection的precision的分母\n",
    "    c_recall_numerator = 0  # Correction的Recall的分子\n",
    "    c_recall_denominator = 0  # Correction的Recall的分母\n",
    "    c_precision_numerator = 0  # Correction的precision的分子\n",
    "    c_precision_denominator = 0  # Correction的precision的分母\n",
    "\n",
    "    prograss = tqdm(range(len(test_data)))\n",
    "    for i in prograss:\n",
    "        src, tgt = test_data[i]['src'], test_data[i]['tgt']\n",
    "\n",
    "        src_tokens = tokenizer(src, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "        tgt_tokens = tokenizer(tgt, return_tensors='pt', max_length=128, truncation=True)['input_ids'][0][1:-1]\n",
    "\n",
    "        # 正常情况下，src和tgt的长度应该是一致的\n",
    "        if len(src_tokens) != len(tgt_tokens):\n",
    "            print(\"第%d条数据异常\" % i)\n",
    "            continue\n",
    "\n",
    "        correction_outputs, _ = model(src)\n",
    "        predict_tokens = correction_outputs[0][1:len(src_tokens) + 1].argmax(1).detach().cpu()\n",
    "\n",
    "        # 计算错误token的数量\n",
    "        d_recall_denominator += (src_tokens != tgt_tokens).sum().item()\n",
    "        # 计算在这些错误token，有多少网络也认为它是错误的\n",
    "        d_recall_numerator += (predict_tokens != src_tokens)[src_tokens != tgt_tokens].sum().item()\n",
    "        # 计算网络找出的错误token的数量\n",
    "        d_precision_denominator += (predict_tokens != src_tokens).sum().item()\n",
    "        # 计算在网络找出的这些错误token中，有多少是真正错误的\n",
    "        d_precision_numerator += (src_tokens != tgt_tokens)[predict_tokens != src_tokens].sum().item()\n",
    "        # 计算Detection的recall、precision和f1-score\n",
    "        d_recall = d_recall_numerator / (d_recall_denominator + 1e-9)\n",
    "        d_precision = d_precision_numerator / (d_precision_denominator + 1e-9)\n",
    "        d_f1_score = 2 * (d_recall * d_precision) / (d_recall + d_precision + 1e-9)\n",
    "\n",
    "        # 计算错误token的数量\n",
    "        c_recall_denominator += (src_tokens != tgt_tokens).sum().item()\n",
    "        # 计算在这些错误token中，有多少网络预测对了\n",
    "        c_recall_numerator += (predict_tokens == tgt_tokens)[src_tokens != tgt_tokens].sum().item()\n",
    "        # 计算网络找出的错误token的数量\n",
    "        c_precision_denominator += (predict_tokens != src_tokens).sum().item()\n",
    "        # 计算网络找出的错误token中，有多少是正确修正的\n",
    "        c_precision_numerator += (predict_tokens == tgt_tokens)[predict_tokens != src_tokens].sum().item()\n",
    "\n",
    "        # 计算Correction的recall、precision和f1-score\n",
    "        c_recall = c_recall_numerator / (c_recall_denominator + 1e-9)\n",
    "        c_precision = c_precision_numerator / (c_precision_denominator + 1e-9)\n",
    "        c_f1_score = 2 * (c_recall * c_precision) / (c_recall + c_precision + 1e-9)\n",
    "\n",
    "        prograss.set_postfix({\n",
    "            'd_recall': d_recall,\n",
    "            'd_precision': d_precision,\n",
    "            'd_f1_score': d_f1_score,\n",
    "            'c_recall': c_recall,\n",
    "            'c_precision': c_precision,\n",
    "            'c_f1_score': c_f1_score,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 90.12it/s, d_recall=0.799, d_precision=0.802, d_f1_score=0.8, c_recall=0.769, c_precision=0.772, c_f1_score=0.771]  \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/test.sighan13.pkl\", mode='br') as f:\n",
    "    sighan13 = pickle.load(f)\n",
    "evaluation(sighan13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:12<00:00, 85.48it/s, d_recall=0.666, d_precision=0.828, d_f1_score=0.738, c_recall=0.643, c_precision=0.799, c_f1_score=0.712]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/test.sighan14.pkl\", mode='br') as f:\n",
    "    sighan14 = pickle.load(f)\n",
    "evaluation(sighan14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:11<00:00, 92.04it/s, d_recall=0.761, d_precision=0.867, d_f1_score=0.811, c_recall=0.725, c_precision=0.827, c_f1_score=0.773] \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/test.sighan15.pkl\", mode='br') as f:\n",
    "    sighan15 = pickle.load(f)\n",
    "evaluation(sighan15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "最后，我们来真正的使用一下该模型，看下效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    sequences = [text]\n",
    "    correction_outputs, _ = model(sequences)\n",
    "    tokens = correction_outputs[0][1:len(text) + 1].argmax(1)\n",
    "    return ''.join(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天早上我吃了一个火聋果'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"今天早上我吃了以个火聋果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是联系时长两年半的个人练习生蔡徐鲲，喜欢唱跳ra##p蓝球[SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"我是联系时长两年半的个人练习生蔡徐鲲，喜欢唱跳RAP蓝球\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "虽然在数据上模型表现还不错，但在真正使用场景上，效果还是不够好。中文文本纠错果然是一个比较难的任务 T_T !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 参考文献\n",
    "\n",
    "[MDCSpell论文](https://aclanthology.org/2022.findings-acl.98/): https://aclanthology.org/2022.findings-acl.98/\n",
    "\n",
    "[MDCSpell论文笔记](https://blog.csdn.net/zhaohongfei_358/article/details/126973451)：https://blog.csdn.net/zhaohongfei_358/article/details/126973451"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}